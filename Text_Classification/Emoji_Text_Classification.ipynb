{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tx07xaiPpSV8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UnW5MzLygxt"
      },
      "outputs": [],
      "source": [
        "class EmojiTextClassifier:\n",
        "    def __init__(self):\n",
        "        self.feature_vectors = {}\n",
        "        self.model = None\n",
        "\n",
        "    def load_dataset(self, train_path, test_path):\n",
        "        df_train = pd.read_csv(train_path)\n",
        "        X_train = np.array(df_train[\"sentence\"])\n",
        "        Y_train = np.array(df_train[\"label\"], dtype=int)\n",
        "\n",
        "        df_test = pd.read_csv(test_path)\n",
        "        X_test = np.array(df_test[\"sentence\"])\n",
        "        Y_test = np.array(df_test[\"label\"], dtype=int)\n",
        "\n",
        "        return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "    def load_feature_vectors(self, link=\"https://nlp.stanford.edu/data/glove.6B.zip\", name=\"glove.6B.50d.txt\"):\n",
        "        print(\"Downloading pre-trained model...\")\n",
        "\n",
        "        if not os.path.exists(\"glove.6B.zip\"):\n",
        "            os.system(f\"wget -q {link}\")\n",
        "\n",
        "        if not os.path.exists(\"glove\"):\n",
        "            os.makedirs(\"glove\", exist_ok=True)\n",
        "            os.system(\"unzip -q glove.6B.zip -d glove\")\n",
        "\n",
        "        path = f\"glove/{name}\"\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\" file '{path}' didn't find! \")\n",
        "\n",
        "        print(\"Pre-trained model âœ…\")\n",
        "\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                word = parts[0]\n",
        "                vector = np.array(parts[1:], dtype=np.float64)\n",
        "                self.feature_vectors[word] = vector\n",
        "\n",
        "    def sentence_to_feature_vectors_avg(self, sentence, D=50):\n",
        "        sentence = sentence.lower()\n",
        "        words = sentence.split()\n",
        "\n",
        "        sum_vectors = np.zeros((D,))\n",
        "        count = 0\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.feature_vectors:\n",
        "                sum_vectors += self.feature_vectors[word]\n",
        "                count += 1\n",
        "\n",
        "        if count == 0:\n",
        "            return sum_vectors\n",
        "        return sum_vectors / count\n",
        "\n",
        "    def vectorize_dataset(self, X_raw, D=50):\n",
        "        return np.array([self.sentence_to_feature_vectors_avg(sent, D) for sent in X_raw])\n",
        "\n",
        "    def train(self, X_train, Y_train, epochs=10, optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], use_dropout=[True, 0.2]):\n",
        "        input_dim = X_train.shape[1]\n",
        "        if use_dropout[0]:\n",
        "            self.model = tf.keras.models.Sequential([\n",
        "                tf.keras.layers.Input(shape=(input_dim,)),\n",
        "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                tf.keras.layers.Dropout(use_dropout[1]),\n",
        "                tf.keras.layers.Dense(5, activation=\"softmax\")\n",
        "            ])\n",
        "        else:\n",
        "            self.model = tf.keras.models.Sequential([\n",
        "                tf.keras.layers.Input(shape=(input_dim,)),\n",
        "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                tf.keras.layers.Dense(5, activation=\"softmax\")\n",
        "            ])\n",
        "\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        self.model.fit(X_train, Y_train, epochs=epochs)\n",
        "\n",
        "    def test(self, X_test, Y_test):\n",
        "        return self.model.evaluate(X_test, Y_test)\n",
        "\n",
        "    def predict(self, sentence, D=50):\n",
        "        vector = self.sentence_to_feature_vectors_avg(sentence, D)\n",
        "        vector = np.expand_dims(vector, axis=0)\n",
        "        prediction = self.model.predict(vector)\n",
        "        return np.argmax(prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo9LCyKnA40i"
      },
      "outputs": [],
      "source": [
        "model_50 = EmojiTextClassifier()\n",
        "X_train_raw, Y_train, X_test_raw, Y_test = model_50.load_dataset(\"dataset/train.csv\", \"dataset/test.csv\")\n",
        "model_50.load_feature_vectors(name=\"glove.6B.50d.txt\")\n",
        "\n",
        "X_train = model_50.vectorize_dataset(X_train_raw,D=50)\n",
        "X_test = model_50.vectorize_dataset(X_test_raw,D=50)\n",
        "\n",
        "model_50.train(X_train, Y_train, epochs=250)\n",
        "model_50.test(X_test, Y_test)\n",
        "\n",
        "result_test = model_50.predict(\"I love programming\",D=50)\n",
        "print(\"predict:\",result_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve2AZB2hGroD"
      },
      "source": [
        "|feature vector dimensions|Train loss|Train accuracy|Test loss|Test accuracy|Inference Time|\n",
        "|-------------------------|----------|--------------|---------|-------------|--------------|\n",
        "|50d|0.1332|0.9647|0.3029|0.8185|138ms|\n",
        "|100d|0.0339|0.9962|0.4719|0.8080|177ms|\n",
        "|200d|0.0070|1.0000|0.4825|0.8423|180ms|\n",
        "|300d|0.0030|1.0000|0.4492|0.8185|188ms|"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
